{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90d1a5fd",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e2aac3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import unicodedata\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6030ee82",
   "metadata": {},
   "source": [
    "## 1. Define a function named basic_clean. It should take in a string and apply some basic text cleaning to it:\n",
    "\n",
    "Lowercase everything\n",
    "Normalize unicode characters\n",
    "Replace anything that is not a letter, number, whitespace or a single quote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "426ea670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(text):\n",
    "    '''This function takes in a string and makes it lowercase, normalizes unicode characters, and\n",
    "        replaces anything that is not a letter, number, whitespace or a single quote.'''\n",
    "    \n",
    "    #lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    #normalize unicode characters\n",
    "    text = unicodedata.normalize('NKFD', text).encode('ascii', 'ignore').decode('utf-8')\n",
    "    \n",
    "    #replace anything not a letter, number, whitespace, or single quote\n",
    "    text = re.sub(r'[^a-z0-9\\s]','', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146aa1f7",
   "metadata": {},
   "source": [
    "## 2. Define a function named tokenize. It should take in a string and tokenize all the words in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e8594a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    '''This function takes in a string and returns it tokenized.'''\n",
    "    \n",
    "    #create the tokenizer\n",
    "    tokenize = nltk.tokenize.ToktokTokenizer()\n",
    "    \n",
    "    #tokenize the text\n",
    "    text = tokenize.tokenize(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceebf1cb",
   "metadata": {},
   "source": [
    "## 3. Define a function named stem. It should accept some text and return the text after applying stemming to all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "345d6062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(text):\n",
    "    '''This function takes in a string and returns it after applying the Porter Stemmer.'''\n",
    "    \n",
    "    #create the stemmer\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    \n",
    "    #apply the stemmer to all words in the string\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e38f26",
   "metadata": {},
   "source": [
    "## 4. Define a function named lemmatize. It should accept some text and return the text after applying lemmatization to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08cb37ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    '''This function takes in a string and returns it after applying WordNet Lemmatizer.'''\n",
    "    \n",
    "    #create the lemmatizer\n",
    "    wnl = ntlk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    #apply the lemmatizer\n",
    "    text = [wnl.lemmatize(word) for word in text]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec91df1c",
   "metadata": {},
   "source": [
    "## 5. Define a function named remove_stopwords. It should accept some text and return the text after removing all the stopwords.\n",
    "\n",
    "This function should define two optional parameters, extra_words and exclude_words. These parameters should define any additional stop words to include, and any words that we don't want to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b6332e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, extra_words=None, exclude_words=None):\n",
    "    '''This function takes in a string and returns the text after removing all stopwords using nltk stopwords list.\n",
    "    There are two optional arguments: extra_words add any additional stop words to the list,\n",
    "                                      exclude_words remove words from the stop list so they will not be removed. '''\n",
    "    \n",
    "    stopwords = stopwords.words('english')\n",
    "    \n",
    "    if extra_words != None:\n",
    "        \n",
    "        stopwords = stopwords.append(extra_words)\n",
    "        \n",
    "    if exclude_words != None:\n",
    "        \n",
    "        stopwords = stopwords.remove(exclude_words)\n",
    "        \n",
    "    text = [word for word in text if word not in stopwords]\n",
    "    \n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a0cfb3",
   "metadata": {},
   "source": [
    "## 7. Make another dataframe for the Codeup blog posts. Name the dataframe codeup_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a24a2a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blog_articles():\n",
    "    '''This function gets five individual urls and\n",
    "    returns a list of dictionaries containing the title and content of each.'''\n",
    "    \n",
    "    #create an empty list to append to\n",
    "    blog_dict_list = []\n",
    "    \n",
    "    #define the urls\n",
    "    url_one = 'https://codeup.edu/featured/apida-heritage-month/'\n",
    "    url_two = 'https://codeup.edu/featured/women-in-tech-rachel-robbins-mayhill/'\n",
    "    url_three = 'https://codeup.edu/codeup-news/women-in-tech-panelist-spotlight-sarah-mellor/'\n",
    "    url_four =  'https://codeup.edu/events/women-in-tech-madeleine/'\n",
    "    url_five = 'https://codeup.edu/codeup-news/panelist-spotlight-4/'\n",
    "    \n",
    "    #define the headers\n",
    "    headers = {'User-Agent': 'Codeup Data Science'}\n",
    "    \n",
    "    #define the responses\n",
    "    response_one = get(url_one, headers = headers)\n",
    "    response_two = get(url_two, headers = headers)\n",
    "    response_three = get(url_three, headers = headers)\n",
    "    response_four = get(url_four, headers = headers)\n",
    "    response_five = get(url_five, headers = headers)\n",
    "    \n",
    "    #define the soups\n",
    "    soup_one = BeautifulSoup(response_one.content, 'html.parser')\n",
    "    soup_two = BeautifulSoup(response_two.content, 'html.parser')\n",
    "    soup_three = BeautifulSoup(response_three.content, 'html.parser')\n",
    "    soup_four = BeautifulSoup(response_four.content, 'html.parser')\n",
    "    soup_five = BeautifulSoup(response_five.content, 'html.parser')\n",
    "    \n",
    "    #define the articles\n",
    "    article_one = soup_one.find('div', class_='entry-content')\n",
    "    article_two = soup_two.find('div', class_='entry-content')\n",
    "    article_three = soup_three.find('div', class_='entry-content')\n",
    "    article_four = soup_four.find('div', class_='entry-content')\n",
    "    article_five = soup_five.find('div', class_='entry-content')\n",
    "    \n",
    "    #make the dicts\n",
    "    blog_one = {'title': soup_one.title.string,\n",
    "            'content': article_one.text,}\n",
    "    blog_two = {'title': soup_two.title.string,\n",
    "            'content': article_two.text,}\n",
    "    blog_three = {'title': soup_three.title.string,\n",
    "            'content': article_three.text,}\n",
    "    blog_four = {'title': soup_four.title.string,\n",
    "            'content': article_four.text,}\n",
    "    blog_five = {'title': soup_five.title.string,\n",
    "            'content': article_five.text,}\n",
    "    \n",
    "    #append to list\n",
    "    blog_dict_list.append(blog_one)\n",
    "    blog_dict_list.append(blog_two)\n",
    "    blog_dict_list.append(blog_three)\n",
    "    blog_dict_list.append(blog_four)\n",
    "    blog_dict_list.append(blog_five)\n",
    "    \n",
    "    return blog_dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67b2bf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "codeup_df = pd.DataFrame(get_blog_articles())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df2efb71",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DataFrame.rename() takes from 1 to 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m codeup_df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m \u001b[43mcodeup_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moriginal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: DataFrame.rename() takes from 1 to 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "codeup_df.columns = codeup_df.rename('content', 'original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fe159a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
